[["index.html", "Breast Canser Chapter 1 Proposal 1.1 Breast Cancer Data 1.2 The modeling goal 1.3 The models for using", " Breast Canser Wang Ruiqi 2023-05-01 Chapter 1 Proposal 1.1 Breast Cancer Data The Original Dataset Link 1.1.1 Describtion This is one of three domains provided by the Oncology Institute that has repeatedly appeared in the machine learning literature. (See also lymphography and primary-tumor.) This data set includes 201 instances of one class and 85 instances of another class. The instances are described by 9 attributes, some of which are linear and some are nominal. 1.1.2 Attribute Information class: no-recurrence-events, recurrence-events age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99. menopose:lt40, ge40, premeno. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39. node-caps:yes, no. deg-malig: 1, 2, 3. breast: left, right. breast-quad: left-up, left-low, right-up, right-low, central. irradiat: yes, no. 1.2 The modeling goal To predict if there is a breast cancer recurrence based on features age,tumor-size, inv-nodes… 1.3 The models for using Logistic Regression KNN Random forest "],["k-nearest-neighbors.html", "Chapter 2 K Nearest Neighbors", " Chapter 2 K Nearest Neighbors ## The prediction error rate is: 0.3454545 ## [1] 0.2909091 0.2727273 0.2545455 0.2727273 0.2545455 0.2727273 0.2545455 ## [8] 0.2545455 0.2545455 0.2727273 ## The prediction error rate is: 0.2545455 "],["logistic-regression.html", "Chapter 3 Logistic Regression 3.1 General Logistic Regression Model 3.2 Logistic Regression Model Based on Single Variable 3.3 Feature Distribution", " Chapter 3 Logistic Regression 3.1 General Logistic Regression Model The second model we are going to use is the Logistic Regression Model. We first build Logistic Regression model based on all the sleceted variables: tumor_zise, age, inv_nodes, deg_malig. The train_dataset: test_dataset = 8:2. ## # weights: 6 (5 variable) ## initial value 153.878674 ## iter 10 value 114.232266 ## final value 114.226396 ## converged ## Call: ## nnet::multinom(formula = class ~ ., data = train.X) ## ## Coefficients: ## Values Std. Err. ## (Intercept) -1.1504766 0.1804155 ## age -0.2245484 0.1664308 ## inv_nodes 0.3202189 0.1626934 ## tumor_size 0.1335999 0.1733618 ## deg_malig 0.7702525 0.1920156 ## ## Residual Deviance: 228.4528 ## AIC: 238.4528 We can conclude the specific model is: \\(p(reoccur|x_1,x_2,x_3,x_4)=e^{-1.1504766-0.2245484x_1+0.3202189x_2+0.1335999x_3+0.7702525x_4}/(1+e^{-1.1504766-0.2245484x_1+0.3202189x_2+0.1335999x_3+0.7702525x_4})\\) ## The prediction error rate is: 0.2363636 From the AIC and Residual Deviance and the error rate on test dataset, we can see the model is not that ideal, and we then visualize the prediction outcomes to check the specific situation. Due to a multifactor logistic regression, here we define \\(temp = -1.1504766-0.2245484x_1+0.3202189x_2+0.1335999x_3+0.7702525x_4\\) We make plot that class versus temp values, then the cut-off line would be \\(temp=0\\), where \\(e^{-1.1504766-0.2245484x_1+0.3202189x_2+0.1335999x_3+0.7702525x_4}=1\\). We can see that when in class noreoccur, the prediction is quite perfect, while all the errors happened in the class reoccur, the prediction is quite different with the truth. Inspired by the partial dependency, we can also plot the distribution of each factor to check the effect of the factor for classification. 3.2 Logistic Regression Model Based on Single Variable In our Multi Factors Logistic model,we take the class noreoccur as the reference, and we can see all these four factors do not have very significantly differences in distributions between two class, the most obvious factor is age, we can see that if the model only judge the class based on age, the prediction would all be class noreoccur, which indicates the age distribution may be very similar in two classes, also we can conclude the cancer reoccurrence rate may have little relation with age. And the best factor we can say from the above figures is inv_nodes, because the model more possibility to predict the sample as class reoccur. While the rest three factors would have less possibility to predict the sample as class reoccur. This tendency of distribution manily leads to that the logistic model based on all four factors may not tell the difference of two classes and have a tendency to predcit the sample as class noreoccur. We can also study the features of correct predictions to prove this. 3.3 Feature Distribution Here we again plot the distribution of different features, and we use different color to represent the correct and wrong predictions, Again, we can see in class reoccur, the correct prediction mostly happened in samples with unique values of factors, specially for feature inv_nodes, the correct predicted sample in class reoccur seems to have far distance with the other samples in class reoccur. "],["random-forest.html", "Chapter 4 Random Forest 4.1 General Random Forest Model 4.2 Reduced Random Forest Model 4.3 Shapley Values 4.4 Partial Dependecy", " Chapter 4 Random Forest 4.1 General Random Forest Model The third model we are going to use is the Random Forest. We first build Random Forest model based on all the variables and the train_dataset: test_dataset = 8:2. Let’s first see the error evolution vs. number of trees of the general model: Where we can see the general trend of the error in model is decreasing, and next we would use the test dataset to obtain the confusion matrix and AUC value of the model to evaluate the performance of the model: ## [1] &quot;Area under curve (AUC) : 0.556&quot; Above is the AUC value of the random forest model, where we can say is not ideal, and the value 0.556 mostly means the model is random somehow, next we would focus on the confusion matrix to tee the specific predicting situation: We can conclude from the confusion matrix that the model still has a preference to predict class reoccur as class noreoccur, more specifically, in predictions, there are 50 samples to be considered as class noreoccur and only 5 samples to be considered as class reoccur.If we want to answer what has lead to this result, we may need to study the features. We focus on the importance of different features: As we can see from the figure above, features tumor_size,age,inv_nodes,deg_malig still are the main contributing factors. 4.2 Reduced Random Forest Model So does this mean if we remove the less influential features and build the model (which means we use the same features like that in model1 and model2) on them would have a better model performance? in other words, we want to know that whether removing these less influential features can reduce the model tendency of predicting samples as class noreoccur. We rebuild the random forest model and see the AUC value and the confusion matrix, ## [1] &quot;Area under curve (AUC) : 0.516&quot; Still, though we have more predictions of class reoccur, the model has a preference to predict class reoccur as class noreoccur. 4.3 Shapley Values For the preference of the model, here we discuss the Shapley values to study how to fairly distribute the “payout” among features. 4.3.1 SHAP plot with one instance We first plot the Shapley values for one instance, Let’s check the prediction of the sample: ## [1] 0.824 We can see the model consider the sample has a probability of 82.4% to be class noreoccur , and take a perspective of all the features, the general trend is that most features make more contributions in predicting class noreoccur, that’s very important for us to explain why the model prefer to predict the samples as class noreoccur. Especially for the feature tumor_size, age and inv_nodes, which are the three most important features for the model, they all have a huge magnitude of the SHAP values and this indicates their strength of the contribution to predict sample as class noreoccur, that’s very important for us to explain why the model think the sample is highly possible to be class noreoccur. 4.3.2 SHAP plot with more instances To study the effect of features deeper, we start to plot the Shapley values for more instances (we use the same instance with previous part), Here we take class noreoccur as the positive reference. Surprisingly, many factors like deg_malig, tumor_size,inv_nodes and age are both contributed a lot to the predictions of both classes. Their huge magnitude of the SHAP values indicates their strength of the contribution which is consistent with the conclusion made previously that they are four most important features of the model. Also, we can see many features don’t have stable performance in SHAP values. Especially,for features age, we can see the shap values of two samples are totally opposite, and both values are quite large which indicates its large contributions to predictions to both classes. Its magnitude of the SHAP values in both negative and positive side indicates their poor difference in sample distribution among 2 classes. We have scatterplot the distribution of each feature among classes, and indeed, all these features have similar distribution in classes. As for the prediction preference of the model, take a perspective of all the features, the general trend is that most features make more contributions in predicting class noreoccur, the several most important features like tumor_size and inv_nodes, it makes more contributions in predicting class noreoccur , for feature age and deg_malig, they probably make more contributions in predicting class reoccur while with a small gap in contributions make to another class. And this is very important for us to explain why the model prefer to predict the samples as class noreoccur. 4.4 Partial Dependecy To verify why different features have such shap value performance, here we study the marginal effect of the one selected numeric feature age and tumor_size by using partial dependency plots. From the PDPs above, first for feature age, the general trend is that the breast cancer recurrence possibility is increasing as age increases. and around age of 60, the risk is at the peak. And when people are old than 60, however the risk is decreasing sharply. it’s easy for us to interpret the increasing risks as age grow, for the human would have a more vulnerable immune system at an older age, while it’s strange to see lower risk around age 70. As for the feature tumor_size, we can see generally, smaller tumor_size leads to a higher risk of breast cancer recurrence, while there are still some fluctuations. And we interpret the trend like, the smaller tumor means more room and potential to grow, leading to a higher chance of breast cancer recurrence. "],["reflections.html", "Chapter 5 Reflections", " Chapter 5 Reflections "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
